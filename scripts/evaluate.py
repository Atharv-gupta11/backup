import os
import json
import pandas as pd
import numpy as np
import cv2
import sys
# Ensure we can import from parent directory
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from main import analyze_post
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Configuration
DATA_DIRS = {
    "REAL": "data/cosmos/real",
    "MISMATCHED": "data/cosmos/mismatched",
    "AI_GEN": "data/challenge/ai_gen"
}

METRICS_FILE = "evaluation_metrics.json"

def add_noise(image_path):
    """Adds random noise to an image for robustness testing."""
    img = cv2.imread(image_path)
    if img is None: return None
    row, col, ch = img.shape
    mean = 0
    var = 0.1
    sigma = var**0.5
    gauss = np.random.normal(mean, sigma, (row, col, ch))
    gauss = gauss.reshape(row, col, ch)
    noisy = img + gauss * 50 # Amplify noise for visibility
    noisy_path = image_path.replace(".jpg", "_noisy.jpg")
    cv2.imwrite(noisy_path, noisy)
    return noisy_path

def evaluate_explanation_quality(explanation):
    """Heuristic check for explanation quality."""
    score = 0
    if len(explanation) > 50: score += 0.4
    if "RISK" in explanation or "SAFE" in explanation or "Likely" in explanation: score += 0.3
    if "consistency" in explanation.lower() or "ai" in explanation.lower() or "context" in explanation.lower(): score += 0.3
    return min(score, 1.0)

def run_evaluation():
    y_true = []
    y_pred = []
    ex_qualities = []
    robustness_passed = 0
    total_robustness_tests = 0
    
    results = []

    print("ðŸš€ Starting Quantitative Evaluation...")

    # 1. Iterate through datasets
    for label_type, folder in DATA_DIRS.items():
        if not os.path.exists(folder):
            print(f"âš ï¸ Skipping missing folder: {folder}")
            continue
            
        files = [f for f in os.listdir(folder) if f.endswith(('.jpg', '.png'))]
        # Limit to 5 samples per category for speed during hackathon demo
        files = files[:5] 
        
        for filename in files:
            img_path = os.path.join(folder, filename)
            
            # Construct Claim
            txt_path = img_path.replace('.jpg', '.txt').replace('.png', '.txt')
            if os.path.exists(txt_path):
                with open(txt_path, 'r') as f:
                    claim = f.read().strip()
            else:
                claim = f"A photo of {filename}" # Fallback
            
            # Ground Truth
            is_misinfo_gt = True if label_type in ["MISMATCHED", "AI_GEN"] else False
            y_true.append(int(is_misinfo_gt))

            # --- NORMAL RUN ---
            report = analyze_post(img_path, claim)
            is_misinfo_pred = report['is_misinfo']
            y_pred.append(int(is_misinfo_pred))
            
            # Explanation Quality
            ex_qualities.append(evaluate_explanation_quality(report['explanation']))

            # --- ROBUSTNESS RUN (Randomly 50% change) ---
            if np.random.rand() > 0.5:
                total_robustness_tests += 1
                noisy_path = add_noise(img_path)
                if noisy_path:
                    report_noisy = analyze_post(noisy_path, claim)
                    # Robustness = Prediction shouldn't flip just because of noise
                    if report_noisy['is_misinfo'] == is_misinfo_pred:
                        robustness_passed += 1
                    try:
                        os.remove(noisy_path)
                    except: pass

            results.append({
                "File": filename,
                "Type": label_type,
                "GroundTruth": "Misinfo" if is_misinfo_gt else "Real",
                "Prediction": "Misinfo" if is_misinfo_pred else "Real",
                "Correct": is_misinfo_gt == is_misinfo_pred
            })
            print(f"Verified {filename} ({label_type}) -> {'âœ…' if is_misinfo_gt == is_misinfo_pred else 'âŒ'}")

    # 2. Calculate Metrics
    if not y_true:
        return {"error": "No data found"}

    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    
    robustness_score = robustness_passed / total_robustness_tests if total_robustness_tests > 0 else 0
    avg_ex_quality = np.mean(ex_qualities) if ex_qualities else 0

    metrics = {
        "accuracy": round(acc, 4),
        "precision": round(prec, 4),
        "recall": round(rec, 4),
        "f1_score": round(f1, 4),
        "robustness": round(robustness_score, 4),
        "explanation_quality": round(avg_ex_quality, 4)
    }

    # Save to file
    with open(METRICS_FILE, 'w') as f:
        json.dump(metrics, f, indent=4)
        
    print("\nðŸ“Š Evaluation Complete!")
    print(json.dumps(metrics, indent=2))
    return metrics

if __name__ == "__main__":
    run_evaluation()
